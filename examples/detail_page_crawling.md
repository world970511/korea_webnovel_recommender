# 상세 페이지 크롤링 가이드

Skyvern으로 상세 페이지를 방문하여 완전한 정보를 수집하는 방법

## 📖 개요

목록 페이지에는 보통 제한적인 정보만 있습니다:
- ❌ 짧게 줄인 줄거리 (50자 정도)
- ❌ 기본 태그 몇 개만
- ❌ 상세 메타데이터 없음

상세 페이지에 들어가면:
- ✅ 완전한 줄거리 (500자 이상)
- ✅ 모든 태그와 키워드
- ✅ 연재 상태, 에피소드 수, 평점, 조회수 등

---

## 🎯 핵심 전략

### 1. 프롬프트에 명시적으로 지시

```python
prompt = """
⭐ 중요: 각 소설의 상세 페이지에 들어가서 완전한 정보를 수집하세요!

단계:
1. 목록에서 소설 제목과 링크 확인
2. 상세 페이지 링크 클릭
3. 상세 페이지에서 모든 정보 수집:
   - 전체 줄거리 (목록의 짧은 버전이 아님)
   - 모든 태그와 키워드
   - 연재 상태, 에피소드 수
   - 평점, 조회수, 좋아요 수
4. 뒤로가기로 목록 페이지로 복귀
5. 다음 소설로 반복
"""
```

### 2. max_steps 조정

상세 페이지 방문은 더 많은 스텝이 필요:

```python
# 기본 크롤링 (목록만)
max_steps = 10

# 상세 페이지 방문
max_steps = limit * 3  # 각 소설당 3 스텝 (목록→상세→복귀)
```

### 3. 확장된 스키마

```python
extraction_schema = {
    # 기본 정보
    "title": "소설 제목",
    "author": "작가 이름",

    # 상세 페이지 전용 정보
    "description": "상세 페이지의 전체 줄거리",
    "keywords": "모든 태그와 키워드",
    "status": "연재 상태 (연재중/완결)",
    "total_episodes": "전체 에피소드 수",
    "rating": "평점",
    "views": "조회수",
    "likes": "좋아요 수",
}
```

---

## 💻 구현 예시

### 예시 1: 기본 상세 페이지 방문

```python
from backend.app.services.crawler.skyvern_client import SkyvernClient

client = SkyvernClient()

prompt = """
네이버 시리즈 판타지 장르 페이지에서 소설 10개를 수집하세요.

중요: 각 소설의 상세 페이지에 들어가서 완전한 정보를 수집!

작업:
1. 목록에서 소설 찾기
2. 제목 클릭하여 상세 페이지 이동
3. 전체 줄거리, 태그, 연재 상태 수집
4. 뒤로가기
5. 다음 소설 반복

정보가 부족하면 상세 페이지에서 확인하세요!
"""

result = await client.run_task(
    url="https://series.naver.com/novel/genre/fantasy",
    prompt=prompt,
    max_steps=30  # 10개 * 3 스텝
)
```

### 예시 2: 조건부 상세 페이지 방문

```python
prompt = """
카카오페이지 로맨스 장르에서 소설 20개를 수집하세요.

규칙:
1. 목록에서 기본 정보 확인
2. 만약 줄거리가 50자 이하로 짧으면:
   → 상세 페이지에 들어가서 전체 줄거리 수집
3. 만약 태그가 2개 이하면:
   → 상세 페이지에서 모든 태그 수집

이렇게 하면 정보가 부족한 소설만 상세 페이지 방문하여 효율적입니다.
"""
```

### 예시 3: 특정 정보만 상세 페이지에서

```python
prompt = """
리디북스 베스트셀러를 수집하세요.

목록 페이지에서:
- 제목, 작가, URL, 간단한 소개

상세 페이지에서만:
- 완전한 줄거리
- 전체 태그 목록
- 평점과 리뷰 수

이렇게 역할을 나누어 작업하세요.
"""
```

---

## 🔍 실전 예시: 네이버 시리즈

### Before (목록만)

```json
{
  "title": "전지적 독자 시점",
  "description": "소설 속 주인공이 되어...",  // 50자만
  "keywords": ["판타지", "현대"],  // 2개만
  "status": null,
  "episodes": null
}
```

### After (상세 페이지 포함)

```json
{
  "title": "전지적 독자 시점",
  "description": "소설 속 주인공이 되어버린 남자. 그는 소설의 결말을 알고 있는 유일한 사람이다. 세계가 멸망하는 것을 막기 위해...",  // 500자 이상
  "keywords": ["판타지", "현대", "회귀", "성장", "게임", "시스템", "생존"],  // 7개
  "status": "연재중",
  "episodes": 551,
  "rating": 9.8,
  "views": 12500000,
  "likes": 325000
}
```

---

## ⚡ 성능 고려사항

### 속도 vs 정보 품질

| 방식 | 속도 | 정보 품질 | 사용 시기 |
|------|------|-----------|-----------|
| 목록만 | 빠름 (20초) | 낮음 | 대량 수집, 기본 정보만 필요 |
| 상세 페이지 | 느림 (2-3분) | 높음 | 정확한 정보, RAG 품질 중요 |
| 조건부 방문 | 중간 (1분) | 중간 | 균형잡힌 접근 |

### 권장 설정

```python
# 빠른 수집 (목록만)
limit = 50
max_steps = 15
visit_detail = False

# 고품질 수집 (상세 페이지)
limit = 20
max_steps = 60  # 20 * 3
visit_detail = True

# 균형잡힌 접근
limit = 30
max_steps = 45
visit_detail_if = "description is short or keywords < 3"
```

---

## 🎨 고급 프롬프트 기법

### 1. 우선순위 기반 수집

```python
prompt = """
소설 20개를 수집하되, 다음 우선순위로:

1순위: 평점 9.0 이상 → 상세 페이지 필수 방문
2순위: 조회수 100만 이상 → 상세 페이지 방문
3순위: 나머지 → 목록 정보만

이렇게 중요한 소설에 더 많은 정보를 수집하세요.
"""
```

### 2. 페이지별 전략

```python
prompt = """
네이버 시리즈 판타지 장르:

1페이지 (인기작):
- 모든 소설 상세 페이지 방문
- 완전한 정보 수집

2-3페이지:
- 평점 높은 것만 상세 페이지
- 나머지는 목록 정보

이렇게 효율적으로 수집하세요.
"""
```

### 3. 에러 핸들링

```python
prompt = """
소설 정보 수집 시:

1. 상세 페이지 링크 클릭 시도
2. 만약 404 에러나 접근 불가:
   → 목록의 정보라도 저장
3. 만약 로그인 필요:
   → 건너뛰고 다음 소설로
4. 만약 로딩이 너무 오래 걸리면:
   → 5초 후 타임아웃하고 다음으로

이렇게 강건하게 처리하세요.
"""
```

---

## 📊 비교: 목록 vs 상세 페이지

### 수집 시간

```
목록만:        ████░░░░░░ (20초)
상세 페이지:   ██████████ (2분)
조건부:        ███████░░░ (1분)
```

### 정보 품질

```
목록만:        ████░░░░░░ (40%)
상세 페이지:   ██████████ (100%)
조건부:        ███████░░░ (70%)
```

### RAG 검색 품질

```
목록만:        ██████░░░░ (60%)
상세 페이지:   ██████████ (95%)
조건부:        ████████░░ (80%)
```

---

## 🚀 실전 사용 방법

### 향상된 크롤러 사용

```bash
# 1. 향상된 크롤러 파일 생성됨
# backend/app/services/crawler/platforms/naver_enhanced.py

# 2. Python에서 사용
python3 << EOF
import asyncio
from backend.app.services.crawler.skyvern_client import SkyvernClient
from backend.app.services.crawler.platforms.naver_enhanced import NaverSeriesCrawlerEnhanced

async def main():
    client = SkyvernClient()
    crawler = NaverSeriesCrawlerEnhanced(client)

    # 상세 페이지 포함 크롤링
    novels = await crawler.crawl_genre_with_details(
        genre="판타지",
        limit=10
    )

    for novel in novels:
        print(f"{novel['title']}")
        print(f"  줄거리: {novel['description'][:100]}...")
        print(f"  상태: {novel.get('status', 'N/A')}")
        print()

asyncio.run(main())
EOF
```

---

## ✅ 체크리스트

상세 페이지 크롤링을 구현할 때:

- [ ] 프롬프트에 "상세 페이지 방문" 명시
- [ ] max_steps를 충분히 설정 (limit * 3 이상)
- [ ] 뒤로가기 지시 포함
- [ ] 상세 페이지 전용 필드 추가 (연재 상태, 평점 등)
- [ ] 에러 핸들링 (404, 타임아웃 등)
- [ ] 속도 vs 품질 트레이드오프 고려

---

## 🎯 결론

**언제 상세 페이지 방문?**

✅ **방문 필요**:
- RAG 검색 품질이 중요할 때
- 완전한 줄거리가 필요할 때
- 메타데이터 (평점, 조회수) 필요할 때
- 소수의 고품질 데이터 수집

❌ **불필요**:
- 대량 수집이 목표일 때
- 기본 정보만으로 충분할 때
- 속도가 중요할 때
- 초기 데이터베이스 구축 단계

**권장**: 처음엔 목록만으로 빠르게 대량 수집 → 나중에 중요한 소설만 상세 정보 보완
